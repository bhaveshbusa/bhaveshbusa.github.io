---
layout: post
title: "From Rule-Based Systems to Reasoning Engines:  Two-Decade Jump in AI"
date: 2025-07-06
---

# From Rule-Based Systems to Reasoning Engines:  Two-Decade Jump in AI


Me
> The 12:35 train leaves at scheduled time from King's Cross to reach Newcastle, but due to signalling problems it was delayed by 55 minutes. At what time did it reach Newcastle? 
> 
> Change the above question with places in India?

Chat GPT
> The 12:35 train leaves at the scheduled time from **New Delhi** to reach **Jaipur**, but due to signalling problems it was delayed by 55 minutes. At what time did it reach Jaipur?

What looks like a basic substitution—just swapping King’s Cross and Newcastle for New Delhi and Jaipur—is actually a small marvel of modern AI. This simple interaction with ChatGPT reveals how far the field has come in handling contextual, grammatical, and cultural nuances.


### **Looking Back: My First Brush with AI**

My journey with AI began over two decades ago when I joined the Knowledge-Based Computer Systems group at the National Center for Software Technology (NCST), now known as CDAC. At the time, I was working on a data mining project—trying to uncover patterns from seemingly unrelated attributes in datasets. It was more about statistics, link analysis, and rule-based inference than anything like today’s models.

Elsewhere in the department, teams were working on language translation, grappling with the complexities of grammar, syntax, and contextual meaning. I remember many hallway conversations that opened my eyes to just how hard language truly is—from handling tenses and idioms to choosing the right word based on context.

Another team was using neural networks to solve planning problems. In those days, compute resources were precious. Training models meant queuing up jobs to run across machines after office hours. There were also parallel efforts in building expert systems and adaptive testing tools. Each of these efforts tackled a narrow, specialised slice of AI.


### **Losing Touch and Regaining Interest**

After transitioning out of academia, I drifted away from the field of AI. While I kept up with occasional research, I remained skeptical about its practical utility. Progress seemed slow and fragmented. 

Then came the recent breakthroughs—especially with Large Language Models (LLMs) like GPT. Suddenly, models could engage in general conversation, write code, answer context-rich queries, and even demonstrate reasoning. For the first time, I began to see real, usable value emerge.


### **What's Really Going On Under the Hood**

Let’s go back to the earlier example:

> The 12:35 train leaves at scheduled time from King's Cross to reach Newcastle, but due to signalling problems it was delayed by 55 minutes. At what time did it reach Newcastle? 

*Change the above question with places in India?*

> "The 12:35 train leaves at the scheduled time from New Delhi to reach Jaipur, but due to signalling problems, it was delayed by 55 minutes. At what time did it reach Jaipur?"_

At first glance, this seems like a trivial change. But there's actually a lot happening behind the scenes:

- **Named Entity Recognition** – understanding that "King’s Cross" and "New Delhi" are both train stations and swapping them appropriately.
    
- **Pattern Recognition** – identifying the sentence structure and preserving its grammatical correctness after modification.
    
- **Contextual Understanding** – maintaining the meaning of the sentence across different cultural geographies.
    
- **Temporal Reasoning** – interpreting the question logically and inferring the correct answer.


On the surface it seems LLM is the magic bullet, however as your can see in this example, there is lot going on under the hood. They are the result of decades of research across various subfields—statistical learning, linguistic modelling, symbolic reasoning, and neural computation—now beautifully interleaved by LLMs.


### **A New Era for Developers and Thinkers**

From a programming perspective, the ability to handle ambiguous inputs and respond with generalised, sensible outputs is a huge leap forward. We're no longer building rules for every possible edge case. Instead, we’re working with models that can _learn the patterns_ and _reason through the context_.


You can follow the conversation with ChatGPT [here](https://chatgpt.com/share/6848b3b6-095c-8010-b133-ff2fe3a5db27)

